# **ğŸ“ PrÃ¡ctica de e-learning: ClasificaciÃ³n con MÃ©todos Ensemble**

## ğŸ¯ Objetivos

- Comprender cÃ³mo funcionan los mÃ©todos ensemble como **Random Forest** y **Gradient Boosting**.
- Implementar modelos ensemble con datos reales.
- Comparar desempeÃ±o con modelos individuales.
- Visualizar la importancia de caracterÃ­sticas y resultados.

## ğŸ“¥ Paso 1: Importar librerÃ­as necesarias

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
```

## ğŸ“š Paso 2: Cargar datos desde repositorio abierto

Usaremos el dataset **"Heart.csv"** desde GitHub para predecir enfermedades cardÃ­acas.

ğŸ”— URL de carga:
 https://raw.githubusercontent.com/ybifoundation/Dataset/main/Heart.csv

```python
url = "https://raw.githubusercontent.com/ybifoundation/Dataset/main/Heart.csv"
df = pd.read_csv(url)
df.head()
```

## ğŸ§¹ Paso 3: PreparaciÃ³n de los datos

```python
# Variables independientes y dependiente
X = df.drop(columns='HeartDisease')
y = df['HeartDisease']

# DivisiÃ³n en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

## ğŸŒ³ Paso 4: Random Forest Classifier

```python
# Modelo Ensemble - Bagging
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# EvaluaciÃ³n
y_pred_rf = rf_model.predict(X_test)
print("Random Forest Classifier")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
```

## âš¡ Paso 5: Gradient Boosting Classifier

```python
# Modelo Ensemble - Boosting
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)

# EvaluaciÃ³n
y_pred_gb = gb_model.predict(X_test)
print("Gradient Boosting Classifier")
print("Accuracy:", accuracy_score(y_test, y_pred_gb))
print(classification_report(y_test, y_pred_gb))
```

## ğŸ“Š Paso 6: Visualizar importancia de caracterÃ­sticas

```python
# Importancia de caracterÃ­sticas en Random Forest
importances = rf_model.feature_importances_
features = X.columns

plt.figure(figsize=(10,6))
sns.barplot(x=importances, y=features)
plt.title("Importancia de CaracterÃ­sticas - Random Forest")
plt.xlabel("Importancia")
plt.ylabel("CaracterÃ­sticas")
plt.grid(True)
plt.show()
```

## âœ… Conclusiones

- Random Forest (bagging) y Gradient Boosting (boosting) combinan mÃºltiples Ã¡rboles para mejorar la predicciÃ³n.
- Ambos mÃ©todos ofrecen **alta precisiÃ³n** y permiten analizar quÃ© variables influyen mÃ¡s.
- Son Ãºtiles en problemas reales donde un solo modelo puede fallar o sobreajustarse.