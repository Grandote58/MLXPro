# ğŸ§ª **PrÃ¡ctica: â€œğŸ§¬ GenFeature: Seleccionando variables con inteligencia evolutivaâ€**

### ğŸ¯ **Objetivo**

Aplicar un **Algoritmo GenÃ©tico** para realizar **selecciÃ³n de caracterÃ­sticas** (feature selection) sobre un dataset real, con el fin de **optimizar el rendimiento de un clasificador** (Random Forest). Se usarÃ¡ la puntuaciÃ³n F1 para evaluar la calidad de las soluciones.

## ğŸ“‚ **Dataset real utilizado**

- ğŸ§  **Nombre:** Breast Cancer Wisconsin (Diagnostic) Data Set
- ğŸ”— **URL de Kaggle:**
   ğŸ‘‰ https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data
- ğŸ“„ Archivo: `data.csv`
- ğŸ“Š Incluye 569 registros de pacientes con 30 caracterÃ­sticas, clasificadas como **malignas o benignas**

## ğŸ‘¨â€ğŸ« **Paso a paso en Google Colab (explicado y con emojis)**

### ğŸ”¹ 1. Importar librerÃ­as necesarias

```python
# ğŸ“š LibrerÃ­as para anÃ¡lisis, ML y evoluciÃ³n
import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
```

### ğŸ”¹ 2. Cargar el dataset

ğŸ”½ **Sube `data.csv` desde Kaggle a Google Colab**

```python
# ğŸ“‚ Cargar datos
df = pd.read_csv('data.csv')
df = df.drop(columns=['id', 'Unnamed: 32'])  # ğŸ§½ Eliminar columnas irrelevantes
df.head()
```

### ğŸ”¹ 3. Preparar datos

```python
# ğŸ¯ Variables predictoras y objetivo
X = df.drop('diagnosis', axis=1)
y = df['diagnosis'].map({'M': 1, 'B': 0})  # Convertimos a binario

# ğŸ§ª DivisiÃ³n de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### ğŸ”¹ 4. FunciÃ³n de evaluaciÃ³n (fitness)

```python
# ğŸ§¬ Evaluar una soluciÃ³n (subset de features)
def evaluate(individual):
    selected = [i for i, bit in enumerate(individual) if bit == 1]
    if not selected: return 0  # âš ï¸ Si no se elige nada, score 0

    model = RandomForestClassifier(n_estimators=50, random_state=42)
    model.fit(X_train.iloc[:, selected], y_train)
    preds = model.predict(X_test.iloc[:, selected])
    return f1_score(y_test, preds)
```

### ğŸ”¹ 5. Crear algoritmo genÃ©tico

```python
# ğŸ”§ ParÃ¡metros bÃ¡sicos
num_features = X.shape[1]
population_size = 10
generations = 20

# ğŸ”¢ Crear poblaciÃ³n inicial
def create_population():
    return [np.random.randint(0, 2, num_features).tolist() for _ in range(population_size)]

# ğŸ’¡ SelecciÃ³n de padres (los dos mejores)
def select_parents(population, scores):
    sorted_pairs = sorted(zip(population, scores), key=lambda x: x[1], reverse=True)
    return [sorted_pairs[0][0], sorted_pairs[1][0]]

# ğŸ”€ Cruce simple
def crossover(p1, p2):
    point = random.randint(1, num_features - 1)
    return p1[:point] + p2[point:]

# ğŸ² MutaciÃ³n aleatoria
def mutate(individual, prob=0.1):
    return [bit if random.random() > prob else 1 - bit for bit in individual]
```

### ğŸ”¹ 6. Ejecutar la evoluciÃ³n

```python
# ğŸ§ª Iniciar poblaciÃ³n
population = create_population()
best_scores = []

for gen in range(generations):
    scores = [evaluate(ind) for ind in population]
    best_scores.append(max(scores))
    
    print(f"ğŸ§¬ GeneraciÃ³n {gen+1} | Mejor F1: {max(scores):.4f}")
    
    parents = select_parents(population, scores)
    offspring = [mutate(crossover(parents[0], parents[1])) for _ in range(population_size - 2)]
    population = parents + offspring
```

### ğŸ”¹ 7. Visualizar evoluciÃ³n del rendimiento

```python
plt.plot(range(1, generations+1), best_scores, marker='o')
plt.title("ğŸ“ˆ EvoluciÃ³n del Mejor F1-score por GeneraciÃ³n")
plt.xlabel("GeneraciÃ³n")
plt.ylabel("F1-score")
plt.grid(True)
plt.show()
```

## ğŸ§  ReflexiÃ³n

1. Â¿QuÃ© tan relevante fue reducir las caracterÃ­sticas?
2. Â¿QuÃ© efecto tiene el tamaÃ±o de la poblaciÃ³n en los resultados?
3. Â¿CÃ³mo influye la tasa de mutaciÃ³n en la exploraciÃ³n del espacio?

## ğŸ“ ConclusiÃ³n

âœ… Aplicaste un Algoritmo GenÃ©tico en un caso real

âœ… Seleccionaste caracterÃ­sticas de forma automÃ¡tica

âœ… Aumentaste el rendimiento del modelo reduciendo variables

âœ… Entendiste cÃ³mo cruzar y mutar soluciones evolutivas

ğŸ§  *â€œLos datos tambiÃ©n evolucionan... cuando tÃº los guÃ­as con inteligencia.â€*