





# **Caso de Estudio** 

## üöë Predicci√≥n de insuficiencia card√≠aca

Nuestro objetivo es construir un modelo que, a partir de datos cl√≠nicos de un paciente, prediga si sufrir√° un evento mortal (insuficiencia card√≠aca).

## **El Objetivo de Aprendizaje üß†:**

No solo calcular m√©tricas, sino **interpretar visualmente** qu√© significa cada una en un contexto donde los errores tienen consecuencias reales.

### **‚úÖ Paso 0: Preparando nuestro Laboratorio Virtual**

Primero, importamos las herramientas que necesitaremos. `pandas` para manejar los datos, `scikit-learn` para el modelo y las m√©tricas, y `matplotlib`/`seaborn` para las visualizaciones.

```python
# Importaci√≥n de librer√≠as esenciales
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    confusion_matrix, 
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score, 
    roc_curve, 
    roc_auc_score,
    classification_report
)

# Estilo visual para nuestros gr√°ficos
plt.style.use('seaborn-v0_8-whitegrid')
```

### **‚úÖ Paso 1: Cargando y Explorando los Datos del Paciente üî¨**

Cargaremos el dataset directamente desde una URL. Esto es genial porque no necesitas descargar nada. Es un dataset p√∫blico y limpio, ideal para aprender.

**Target a Predecir:** DEATH_EVENT **(0 = No muri√≥, 1 = Muri√≥)**

```python
# URL del dataset en formato raw
url = 'https://raw.githubusercontent.com/dataspelunking/MLwHeartFailure/main/heart_failure_clinical_records_dataset.csv'

# Cargar los datos en un DataFrame de pandas
df_pacientes = pd.read_csv(url)

# Echemos un primer vistazo a los datos de nuestros pacientes
print("Primeros 5 registros de pacientes:")
display(df_pacientes.head())

# üìä ¬°Importante! Verifiquemos el balance de clases
print("\nDistribuci√≥n de la variable objetivo (DEATH_EVENT):")
print(df_pacientes['DEATH_EVENT'].value_counts())
sns.countplot(x='DEATH_EVENT', data=df_pacientes)
plt.title('Distribuci√≥n de Clases: ¬øDataset Balanceado?')
plt.show()
```

> **Interpretaci√≥n:** Vemos que el dataset est√° **desbalanceado**. Hay muchos m√°s pacientes que sobrevivieron (Clase 0) que los que no (Clase 1). 
>
> ¬°Esto hace que la m√©trica de 'Exactitud' por s√≠ sola sea **peligrosa** y nos obliga a usar otras m√©tricas!

### ‚úÖ Paso 2: Pre-procesamiento y Entrenamiento del Modelo ‚ù§Ô∏è

Vamos a preparar los datos y entrenar un modelo simple pero efectivo: la Regresi√≥n Log√≠stica.

1. **Separar** caracter√≠sticas (X) de la etiqueta que queremos predecir (y).
2. **Dividir** los datos en un set de entrenamiento (para que el modelo aprenda) y un set de prueba (para evaluarlo con datos que nunca ha visto).
3. **Entrenar** el modelo.



```python
# 1. Separar caracter√≠sticas (X) y objetivo (y)
X = df_pacientes.drop('DEATH_EVENT', axis=1)
y = df_pacientes['DEATH_EVENT']

# 2. Dividir en datos de entrenamiento y prueba (80% / 20%)
# Usamos 'stratify=y' para mantener la misma proporci√≥n de clases en ambos sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)

# 3. Escalar los datos es una buena pr√°ctica para muchos modelos
# (No lo haremos aqu√≠ para mantener la simplicidad, pero es importante saberlo)

# 4. Crear y entrenar el modelo de Regresi√≥n Log√≠stica
modelo = LogisticRegression(max_iter=1000, random_state=42) # Aumentamos max_iter para asegurar convergencia
modelo.fit(X_train, y_train)
```

### ‚úÖ Paso 3: Obteniendo las Predicciones üéØ

Ahora que el modelo est√° entrenado, lo usaremos para predecir los resultados en nuestro conjunto de prueba.

```python
# El modelo ahora predice sobre los datos de prueba
y_pred = modelo.predict(X_test)

# Tambi√©n obtenemos las probabilidades de predicci√≥n para la curva ROC
y_pred_proba = modelo.predict_proba(X_test)[:, 1] # Probabilidad de la clase '1'

# Guardemos los valores reales y predichos para facilitar el acceso
valores_reales = y_test
predicciones = y_pred
```

### ‚úÖ Paso 4: ¬°La Hora de la Verdad! M√©trica por M√©trica üìä

Aqu√≠ es donde interrogamos a nuestro modelo.

#### **4.1 Matriz de Confusi√≥n: El Mapa de la Verdad**

Este es el punto de partida de todo. 

Nos muestra los 4 posibles resultados de una predicci√≥n binaria.

```python
# Calcular la matriz de confusi√≥n
cm = confusion_matrix(valores_reales, predicciones)

# Extraer los valores para una mejor explicaci√≥n
TN, FP, FN, TP = cm.ravel()

print(f"Verdaderos Negativos (TN): {TN} -> El modelo predijo 'No Muerte' y acert√≥.")
print(f"Falsos Positivos (FP): {FP} -> El modelo predijo 'Muerte' pero el paciente sobrevivi√≥. (Falsa Alarma üö®)")
print(f"Falsos Negativos (FN): {FN} -> El modelo predijo 'No Muerte' pero el paciente muri√≥. (¬°El peor error! üöë)")
print(f"Verdaderos Positivos (TP): {TP} -> El modelo predijo 'Muerte' y acert√≥.")

# Visualizaci√≥n gr√°fica de la Matriz de Confusi√≥n
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Predicho: No Muerte', 'Predicho: Muerte'], 
            yticklabels=['Real: No Muerte', 'Real: Muerte'])
plt.title('Matriz de Confusi√≥n: El Mapa de Aciertos y Errores', fontsize=15)
plt.ylabel('Valor Real')
plt.xlabel('Valor Predicho')
plt.show()
```

> **Interpretaci√≥n Visual:** El gr√°fico nos muestra claramente los aciertos en la diagonal principal (azul oscuro). Vemos que hay **5 Falsos Negativos**, que son los casos m√°s cr√≠ticos que debemos intentar reducir.

#### **4.2 Exactitud (Accuracy): La M√©trica General**

> ¬øQu√© porcentaje total de predicciones fue correcto?

```python
# Calcular la exactitud
accuracy = accuracy_score(valores_reales, predicciones)
print(f"üéØ Exactitud (Accuracy): {accuracy:.2%}")

# Gr√°fico interpretativo
fig, ax = plt.subplots(figsize=(7, 3))
ax.barh(['Predicciones Correctas'], [accuracy], color='cornflowerblue', label=f'{accuracy:.2%}')
ax.barh(['Predicciones Incorrectas'], [1 - accuracy], left=[accuracy], color='salmon', label=f'{1 - accuracy:.2%}')
ax.set_xlim(0, 1)
ax.set_xticks([])
ax.set_title("Desglose de la Exactitud")
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
```

> **Interpretaci√≥n:** Un 81.67% de exactitud parece bueno, pero ¬°cuidado! Dado el desbalance de clases, esta m√©trica puede ser enga√±osa. Necesitamos investigar m√°s a fondo.

#### **4.3 Precisi√≥n (Precision): La Calidad de las Alertas**

> De todas las veces que predijimos "Muerte", ¬øqu√© porcentaje fue correcto?

```python
# Calcular la precisi√≥n
precision = precision_score(valores_reales, predicciones)
print(f"üîç Precisi√≥n (Precision): {precision:.2%}")
print("Esta m√©trica nos dice qu√© tan confiables son nuestras alarmas de 'Muerte'. Un 70% significa que de 10 alarmas, 3 son falsas.")
```

#### **4.4 Exhaustividad (Recall / Sensibilidad): El Poder de Detecci√≥n**

> De todos los pacientes que *realmente* murieron, ¬øa cu√°ntos logramos detectar?

```python
# Calcular la exhaustividad (recall)
recall = recall_score(valores_reales, predicciones)
print(f"‚ù§Ô∏è Exhaustividad (Recall / Sensibilidad): {recall:.2%}")
print("¬°Esta es una m√©trica CR√çTICA aqu√≠! Nos dice que solo encontramos al 74% de los pacientes en riesgo. ¬°El 26% restante son Falsos Negativos!")
```

#### **4.5 Puntuaci√≥n F1 (F1-Score): El Equilibrio**

> Una media arm√≥nica que combina Precisi√≥n y Recall. √ötil cuando las clases est√°n desbalanceadas.

```python
# Calcular el F1-Score
f1 = f1_score(valores_reales, predicciones)
print(f"‚öñÔ∏è Puntuaci√≥n F1 (F1-Score): {f1:.2%}")
print("Nos da un solo n√∫mero que balancea la precisi√≥n y el recall. Es una mejor medida general que la exactitud para este problema.")
```

> **Gr√°fico Interpretativo (Precisi√≥n, Recall, F1):**

```python
# Gr√°fico comparativo de las m√©tricas clave
metrics_dict = {'Precisi√≥n': precision, 'Recall': recall, 'F1-Score': f1}
metrics_df = pd.DataFrame.from_dict(metrics_dict, orient='index', columns=['Puntuaci√≥n'])

ax = metrics_df.plot(kind='bar', figsize=(8, 5), legend=False, colormap='viridis')
plt.title('Comparativa: Precisi√≥n vs. Recall vs. F1-Score')
plt.ylabel('Puntuaci√≥n')
plt.xticks(rotation=0)
ax.bar_label(ax.containers[0], fmt='%.2f')
plt.show()
```

> **Interpretaci√≥n:** Este gr√°fico nos permite ver el famoso **trade-off**. Tenemos un Recall decente pero una Precisi√≥n un poco m√°s baja. El F1-Score nos da un valor intermedio que resume este balance.

#### **4.6 Especificidad (Specificity): Identificando a los Sanos**

> De todos los pacientes que *realmente* estaban bien (no murieron), ¬øa cu√°ntos clasificamos correctamente?

```python
# La especificidad se calcula a partir de la matriz de confusi√≥n: TN / (TN + FP)
specificity = TN / (TN + FP)
print(f"üõ°Ô∏è Especificidad (Specificity): {specificity:.2%}")
print("Nuestro modelo es bastante bueno (85%) identificando a los pacientes que no van a tener un evento mortal.")
```

#### **4.7 Curva ROC y AUC: La Visi√≥n Panor√°mica**

Esta curva muestra c√≥mo se comporta el modelo en **todos los posibles umbrales de decisi√≥n**. El √°rea bajo la curva (AUC) nos da una medida global de rendimiento.

```python
# Calcular FPR, TPR para la curva ROC
fpr, tpr, thresholds = roc_curve(valores_reales, y_pred_proba)
# Calcular el AUC
auc = roc_auc_score(valores_reales, y_pred_proba)

# Graficar la Curva ROC
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Clasificador Aleatorio')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos (1 - Especificidad)')
plt.ylabel('Tasa de Verdaderos Positivos (Recall / Sensibilidad)')
plt.title('Curva ROC: Rendimiento General del Clasificador', fontsize=15)
plt.legend(loc="lower right")
plt.show()
```

> **Interpretaci√≥n Visual:** Nuestro modelo (l√≠nea naranja) est√° significativamente por encima de la l√≠nea de azar (azul punteada). Un AUC de 0.88 es bastante robusto e indica que el modelo tiene un buen poder de discriminaci√≥n entre las dos clases. Cuanto m√°s se acerque la curva a la esquina superior izquierda, mejor.

### ‚úÖ Paso 5: Resumen y Conclusiones Finales üéì

Scikit-learn nos da una herramienta fant√°stica para ver un resumen de todo.

```python
# Reporte de clasificaci√≥n completo
reporte = classification_report(valores_reales, predicciones, target_names=['No Muerte', 'Muerte'])
print("üìã Reporte de Clasificaci√≥n Completo:")
print(reporte)
```

**Conclusi√≥n :**

Hemos pasado de una simple "Exactitud" a un entendimiento profundo del comportamiento de nuestro modelo.

Sabemos que es bueno identificando a los sanos (**alta Especificidad**), pero tiene un margen de mejora para encontrar a todos los enfermos (**Recall del 74%**).

Dependiendo del objetivo (¬øminimizar falsas alarmas o no dejar escapar a ning√∫n enfermo?), podr√≠amos ajustar el umbral de decisi√≥n del modelo para optimizar la Precisi√≥n o el Recall.

