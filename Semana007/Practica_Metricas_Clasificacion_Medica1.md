
# **ğŸ§ª MÃ©tricas de ClasificaciÃ³n con Datos MÃ©dicos**

ğŸ‘©â€âš•ï¸ *Ãrea: Medicina* 

ğŸ¤– *Tema: EvaluaciÃ³n de Modelos de ClasificaciÃ³n*

### ğŸ“ *Formato: e-Learning para plataformas LMS / Typora / Google Colab*

## ğŸ¯ Objetivo

Aplicar y comprender las principales mÃ©tricas de evaluaciÃ³n en modelos de clasificaciÃ³n binaria:
- Matriz de ConfusiÃ³n
- Exactitud (Accuracy)
- PrecisiÃ³n (Precision)
- Exhaustividad (Recall)
- F1 Score
- Especificidad
- Curva ROC y AUC



## ğŸ”— Paso 1: Cargar los datos mÃ©dicos

```python
import pandas as pd

url = 'https://raw.githubusercontent.com/ageron/data/main/heart_disease/heart.csv'
datos = pd.read_csv(url)
datos.head()
```

## âœ‚ï¸ Paso 2: Preparar y dividir el conjunto de datos

```python
from sklearn.model_selection import train_test_split

X = datos.drop('target', axis=1)
y = datos['target']

X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, test_size=0.25, random_state=42)
```

## ğŸ¤– Paso 3: Entrenar un modelo

```python
from sklearn.linear_model import LogisticRegression

modelo = LogisticRegression(max_iter=1000)
modelo.fit(X_entrenamiento, y_entrenamiento)
```

## ğŸ§© Paso 4: Matriz de ConfusiÃ³n

```python
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

y_pred = modelo.predict(X_prueba)
ConfusionMatrixDisplay.from_estimator(modelo, X_prueba, y_prueba, cmap='Blues')
plt.title('ğŸ“Š Matriz de ConfusiÃ³n')
plt.grid(False)
plt.show()
```

## âœ… Paso 5: Exactitud (Accuracy)

```python
from sklearn.metrics import accuracy_score

exactitud = accuracy_score(y_prueba, y_pred)
print(f"ğŸ¯ Exactitud del modelo: {exactitud:.2%}")
```

## ğŸ¯ Paso 6: PrecisiÃ³n (Precision)

```python
from sklearn.metrics import precision_score

precision = precision_score(y_prueba, y_pred)
print(f"ğŸ” PrecisiÃ³n del modelo: {precision:.2%}")
```

## ğŸ” Paso 7: Exhaustividad (Recall)

```python
from sklearn.metrics import recall_score

recall = recall_score(y_prueba, y_pred)
print(f"ğŸ“ˆ Exhaustividad del modelo: {recall:.2%}")
```

## âš–ï¸ Paso 8: F1 Score

```python
from sklearn.metrics import f1_score

f1 = f1_score(y_prueba, y_pred)
print(f"ğŸ”— F1 Score del modelo: {f1:.2%}")
```

## ğŸ§ª Paso 9: Especificidad

```python
from sklearn.metrics import confusion_matrix

matriz = confusion_matrix(y_prueba, y_pred)
tn, fp, fn, tp = matriz.ravel()
especificidad = tn / (tn + fp)
print(f"ğŸ§¬ Especificidad del modelo: {especificidad:.2%}")
```

## ğŸ“‰ Paso 10: Curva ROC y AUC

```python
from sklearn.metrics import roc_curve, roc_auc_score

y_scores = modelo.predict_proba(X_prueba)[:, 1]
fpr, tpr, _ = roc_curve(y_prueba, y_scores)
auc = roc_auc_score(y_prueba, y_scores)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, label=f'ROC (AUC = {auc:.2f})', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('ğŸ“‰ Curva ROC - EvaluaciÃ³n del Modelo')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
```

## ğŸ§  ReflexiÃ³n Final

Cada mÃ©trica entrega una **perspectiva distinta** sobre el comportamiento del modelo:

- **Exactitud**: bien cuando las clases estÃ¡n balanceadas.
- **PrecisiÃ³n**: importante si los falsos positivos son costosos.
- **Recall**: clave si los falsos negativos son inaceptables.
- **F1 Score**: balance entre precisiÃ³n y recall.
- **Especificidad**: evita clasificar sanos como enfermos.
- **AUC ROC**: mide quÃ© tan bien el modelo discrimina clases en todos los umbrales.

