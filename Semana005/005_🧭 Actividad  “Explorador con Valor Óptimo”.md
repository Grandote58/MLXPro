# ğŸ§­ **Actividad : â€œExplorador con Valor Ã“ptimoâ€**

## ğŸ¯ **Objetivos de Aprendizaje**

1. Comprender el mÃ©todo de **Value Iteration** aplicado a problemas de decisiÃ³n secuencial.
2. Implementar el cÃ¡lculo de valores Ã³ptimos y derivar la **polÃ­tica Ã³ptima**.
3. Utilizar un dataset real como base para simular decisiones de un agente.
4. Visualizar la **convergencia del algoritmo** y la estrategia del agente.

## ğŸ§° **LibrerÃ­as necesarias**

```python
!pip install pandas numpy matplotlib --quiet
```

ğŸ§° **LibrerÃ­as necesarias**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

## ğŸ“¦ **Dataset real utilizado**

ğŸ“„ Dataset: [Metro Interstate Traffic Volume â€“ UCI](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume)

Este dataset contiene registros reales del volumen de trÃ¡fico. Lo usaremos para modelar un entorno tipo lÃ­nea de decisiÃ³n (MDP secuencial), donde el agente debe decidir si **avanza** o **espera** en funciÃ³n del trÃ¡fico y obtener la polÃ­tica Ã³ptima con **Value Iteration**.

## ğŸªœ **Paso a Paso Detallado**

### ğŸ”¹ Paso 1: Cargar y preparar los datos

```python
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv"
df = pd.read_csv(url)
df_sample = df[['traffic_volume']].head(10).copy()
df_sample['state'] = [f"S{i}" for i in range(len(df_sample))]
df_sample.reset_index(drop=True, inplace=True)
df_sample
```

### ğŸ”¹ Paso 2: Inicializar las variables del modelo MDP

```python
states = df_sample['state'].tolist()
actions = ['avanzar', 'esperar']
gamma = 0.9
threshold = 1e-4

# InicializaciÃ³n de valores y recompensas
V = {s: 0 for s in states}
rewards = {}

for i, row in df_sample.iterrows():
    if row['traffic_volume'] < 4000:
        rewards[row['state']] = 1  # recompensa por avanzar con trÃ¡fico bajo
    elif row['traffic_volume'] < 6000:
        rewards[row['state']] = 0  # trÃ¡fico medio, sin penalizaciÃ³n
    else:
        rewards[row['state']] = -1  # penalizaciÃ³n por trÃ¡fico alto
```

### ğŸ”¹ Paso 3: Implementar Value Iteration

```python
deltas = []
iterations = 0

while True:
    delta = 0
    new_V = {}
    for i, s in enumerate(states):
        q_sa = []
        for a in actions:
            if a == 'esperar':
                r = -0.2  # costo de esperar
                next_state = s
            else:
                r = rewards[s]
                next_state = states[i+1] if i+1 < len(states) else s
            q = r + gamma * V[next_state]
            q_sa.append(q)
        best_q = max(q_sa)
        new_V[s] = best_q
        delta = max(delta, abs(V[s] - best_q))
    V = new_V
    deltas.append(delta)
    iterations += 1
    if delta < threshold:
        break

print(f"âœ… Convergencia en {iterations} iteraciones")
```

### ğŸ”¹ Paso 4: Derivar la polÃ­tica Ã³ptima

```python
policy = {}
for i, s in enumerate(states):
    q_sa = {}
    for a in actions:
        if a == 'esperar':
            r = -0.2
            next_state = s
        else:
            r = rewards[s]
            next_state = states[i+1] if i+1 < len(states) else s
        q_sa[a] = r + gamma * V[next_state]
    policy[s] = max(q_sa, key=q_sa.get)

# Mostrar polÃ­tica
pd.DataFrame.from_dict(policy, orient='index', columns=['Mejor AcciÃ³n'])
```

### ğŸ”¹ Paso 5: Visualizar la convergencia del algoritmo

```python
plt.plot(deltas)
plt.title("ğŸ“‰ Convergencia de Value Iteration")
plt.xlabel("Iteraciones")
plt.ylabel("Delta (mÃ¡xima diferencia)")
plt.grid(True)
plt.show()
```

## ğŸ“ **ReflexiÃ³n **

> ğŸ§  En esta actividad, el agente explorador aprendiÃ³ a **valorar estados** y tomar decisiones Ã³ptimas con base en datos reales.
>  ğŸ” Value Iteration permitiÃ³ calcular el mejor camino en funciÃ³n del trÃ¡fico y el costo de esperar.
>  ğŸ›£ï¸ Este modelo se puede escalar a mapas urbanos, planificaciÃ³n logÃ­stica y sistemas de movilidad inteligente.