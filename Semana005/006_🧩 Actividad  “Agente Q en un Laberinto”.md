# ğŸ§© **Actividad : â€œAgente Q en un Laberintoâ€**

## ğŸ¯ **Objetivos de Aprendizaje**

1. Comprender e implementar el algoritmo **Q-Learning** en entornos estocÃ¡sticos.
2. Entrenar un agente que mejore su comportamiento a travÃ©s de la experiencia.
3. Visualizar la evoluciÃ³n del aprendizaje mediante curvas de recompensa.
4. Derivar una **polÃ­tica Ã³ptima** a partir de la Q-table.

## ğŸ§° **LibrerÃ­as necesarias**

```python
!pip install gymnasium matplotlib numpy --quiet
```

ğŸ§° **LibrerÃ­as necesarias**

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import random
from IPython.display import clear_output
```

## ğŸ—‚ï¸ **Dataset / Entorno**

Utilizamos el entorno `FrozenLake-v1` del repositorio Gymnasium (Farama Foundation):

 ğŸ”— [DocumentaciÃ³n oficial](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)

Este entorno es una grilla de 4x4 donde el agente debe llegar a una meta sin caer en los agujeros.

## ğŸªœ **Paso a Paso Detallado**

### ğŸ”¹ Paso 1: Inicializar el entorno

```python
env = gym.make("FrozenLake-v1", is_slippery=False)
state_space = env.observation_space.n
action_space = env.action_space.n
print(f"ğŸ¯ Estados: {state_space} | ğŸ® Acciones: {action_space}")
```

### ğŸ”¹ Paso 2: Crear la tabla Q

```python
q_table = np.zeros((state_space, action_space))
print("ğŸ“‹ Tabla Q inicial:")
print(q_table)
```

### ğŸ”¹ Paso 3: Definir los hiperparÃ¡metros

```python
alpha = 0.8        # tasa de aprendizaje
gamma = 0.95       # descuento de futuro
epsilon = 1.0      # nivel de exploraciÃ³n inicial
min_epsilon = 0.01
decay_rate = 0.005
episodes = 1000
max_steps = 100
rewards = []
```

### ğŸ”¹ Paso 4: Entrenar al agente

```python
for ep in range(episodes):
    state, _ = env.reset()
    total_reward = 0

    for _ in range(max_steps):
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # exploraciÃ³n
        else:
            action = np.argmax(q_table[state])

        next_state, reward, terminated, truncated, _ = env.step(action)

        # ActualizaciÃ³n Q-Learning
        q_table[state, action] = q_table[state, action] + alpha * (
            reward + gamma * np.max(q_table[next_state]) - q_table[state, action])

        state = next_state
        total_reward += reward

        if terminated or truncated:
            break

    epsilon = max(min_epsilon, epsilon * np.exp(-decay_rate * ep))
    rewards.append(total_reward)
```

### ğŸ”¹ Paso 5: Visualizar recompensas por episodio

```python
plt.plot(rewards)
plt.title("ğŸ“ˆ Recompensa por episodio")
plt.xlabel("Episodio")
plt.ylabel("Recompensa")
plt.grid(True)
plt.show()
```

### ğŸ”¹ Paso 6: Derivar la polÃ­tica aprendida

```python
actions_map = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}
policy = np.array([actions_map[np.argmax(row)] for row in q_table])
print("ğŸ§­ PolÃ­tica aprendida:")
print(policy.reshape(4, 4))
```

## ğŸ“ **ReflexiÃ³n**

> â€œEste ejercicio demuestra cÃ³mo un agente puede aprender **por sÃ­ mismo** sin conocer previamente el entorno. A travÃ©s de la exploraciÃ³n y actualizaciÃ³n de la tabla Q, el agente construye su **propia estrategia Ã³ptima**.
>
> Esta lÃ³gica es aplicable a robÃ³tica, videojuegos, movilidad urbana e inteligencia autÃ³noma.
>  Â¡Y lo mejor es que has construido un agente que tomÃ³ decisiones inteligentes desde cero! ğŸ§ ğŸâ€