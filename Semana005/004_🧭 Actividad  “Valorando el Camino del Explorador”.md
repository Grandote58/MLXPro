# **ğŸ§­ Actividad : â€œValorando el Camino del Exploradorâ€**

## ğŸ¯ Objetivos de Aprendizaje

1. Comprender los conceptos de **polÃ­tica**, **valor de estado** y **valor de acciÃ³n** en un entorno realista.
2. Aplicar estos conceptos utilizando un **dataset real** como base para modelar decisiones secuenciales.
3. Visualizar las funciones de valor con grÃ¡ficas interpretables.
4. Desarrollar un pensamiento crÃ­tico sobre decisiones Ã³ptimas bajo incertidumbre.

## ğŸ§° LibrerÃ­as necesarias

```python
!pip install pandas matplotlib seaborn --quiet
```

ğŸ§° LibrerÃ­as necesarias

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
```

## ğŸ—‚ï¸ Dataset utilizado

ğŸ“„ Dataset: **Metro Interstate Traffic Volume**
 ğŸ”— [UCI Repository](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume)
 ğŸ“¥ Descarga directa:

```python
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv"
```

Este dataset contiene datos del trÃ¡fico urbano. Utilizaremos el volumen de trÃ¡fico para simular un entorno secuencial donde un agente debe decidir entre â€œesperarâ€ o â€œavanzarâ€.

## ğŸªœ Paso a Paso Detallado

### ğŸ”¹ Paso 1: Cargar y preparar los datos

```python
df = pd.read_csv(url)
df_sample = df[['holiday', 'traffic_volume']].head(10).copy()
df_sample['state'] = [f'S{i}' for i in range(len(df_sample))]
df_sample.reset_index(drop=True, inplace=True)
df_sample
```

### ğŸ”¹ Paso 2: Definir la polÃ­tica simple (Ï€) y recompensas

```python
# PolÃ­tica simple basada en trÃ¡fico
df_sample['action'] = df_sample['traffic_volume'].apply(lambda x: 'avanzar' if x < 4000 else 'esperar')

# Recompensa simulada: +1 por avanzar con poco trÃ¡fico, -1 por avanzar con mucho trÃ¡fico, 0 por esperar
def calcular_recompensa(row):
    if row['action'] == 'avanzar':
        return 1 if row['traffic_volume'] < 4000 else -1
    else:
        return 0

df_sample['reward'] = df_sample.apply(calcular_recompensa, axis=1)
df_sample[['state', 'action', 'reward']]
```

### ğŸ”¹ Paso 3: Calcular la funciÃ³n de valor V(s)

```python
gamma = 0.9  # factor de descuento
df_sample['V'] = 0.0

# Se calcula hacia atrÃ¡s para simular planificaciÃ³n
for i in reversed(range(len(df_sample))):
    r = df_sample.loc[i, 'reward']
    v_next = df_sample.loc[i+1, 'V'] if i+1 < len(df_sample) else 0
    df_sample.loc[i, 'V'] = r + gamma * v_next

df_sample[['state', 'action', 'reward', 'V']]
```

### ğŸ”¹ Paso 4: Calcular la funciÃ³n de acciÃ³n-valor Q(s,a)

```python
Q = []
for i in range(len(df_sample)):
    s = df_sample.loc[i, 'state']
    for a in ['avanzar', 'esperar']:
        if a == 'esperar':
            r = 0
            v = gamma * df_sample.loc[i, 'V']
        else:
            r = 1 if df_sample.loc[i, 'traffic_volume'] < 4000 else -1
            v = gamma * df_sample.loc[i+1, 'V'] if i+1 < len(df_sample) else 0
        Q.append({'state': s, 'action': a, 'Q_value': r + v})

df_q = pd.DataFrame(Q)
df_q.pivot(index='state', columns='action', values='Q_value')
```

### ğŸ”¹ Paso 5: Visualizar con un heatmap ğŸ§¯

```python
plt.figure(figsize=(10,4))
pivot_q = df_q.pivot(index='state', columns='action', values='Q_value')
sns.heatmap(pivot_q, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("ğŸ”¢ FunciÃ³n Q(s,a): Valor esperado por acciÃ³n")
plt.show()
```

## ğŸ“ ReflexiÃ³n 

> â€œHas usado datos reales para calcular cuÃ¡nto vale cada estado y cada acciÃ³n posible en Ã©l. Â¡Eso es inteligencia artificial aplicada! Esta es la base que usan sistemas autÃ³nomos para actuar de forma Ã³ptima en entornos reales. ğŸš¦ğŸ¤–â€