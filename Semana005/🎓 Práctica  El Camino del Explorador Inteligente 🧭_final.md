# ğŸ“ **PrÃ¡ctica : El Camino del Explorador Inteligente ğŸ§­**

## ğŸ¯ **Objetivo General**

Guiar paso a paso en la comprensiÃ³n del equilibrio entre **exploraciÃ³n** y **explotaciÃ³n** usando un entorno visual y experimental de aprendizaje por reforzamiento.

## ğŸ§  **Contexto de Aprendizaje**

Imagina que eres un explorador ğŸ§­ en una isla misteriosa. Tienes que encontrar la salida (la meta ğŸ) evitando caer en trampas ğŸ•³ï¸. Solo sabrÃ¡s si un camino es bueno al probarloâ€¦ pero no puedes perder tiempo tomando malas decisiones.

AsÃ­ funciona el **Q-Learning**: tu agente irÃ¡ aprendiendo cuÃ¡les decisiones tomar explorando al principio y explotando su conocimiento cuando estÃ© mÃ¡s entrenado.

## ğŸ“¦ **LibrerÃ­as necesarias**

```python
!pip install gymnasium numpy matplotlib --quiet

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import random
```

## ğŸ§ª **Entorno**

Usaremos el entorno `FrozenLake-v1` de Gymnasium (4x4, sin resbalones), ideal para visualizar decisiones en un entorno cuadrado.

```python
env = gym.make("FrozenLake-v1", is_slippery=False)
state_space = env.observation_space.n
action_space = env.action_space.n
print(f"ğŸŒ Estados: {state_space}, ğŸ® Acciones posibles: {action_space}")
```

## ğŸ—ºï¸ **Paso 1: Inicializar Q-Table y parÃ¡metros**

```python
q_table = np.zeros((state_space, action_space))

# HiperparÃ¡metros
alpha = 0.8          # tasa de aprendizaje
gamma = 0.95         # descuento de recompensa futura
epsilon = 1.0        # nivel de exploraciÃ³n inicial
min_epsilon = 0.01
decay_rate = 0.005

episodes = 500
max_steps = 100

rewards = []
epsilon_values = []
```

## ğŸ¤– **Paso 2: Entrenar al Explorador Q**

```python
for ep in range(episodes):
    state, _ = env.reset()
    total_reward = 0

    for _ in range(max_steps):
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # ğŸ” ExploraciÃ³n
        else:
            action = np.argmax(q_table[state])  # ğŸ§  ExplotaciÃ³n

        new_state, reward, terminated, truncated, _ = env.step(action)
        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[new_state]) - q_table[state, action])
        state = new_state
        total_reward += reward

        if terminated or truncated:
            break

    epsilon = max(min_epsilon, epsilon * np.exp(-decay_rate * ep))
    epsilon_values.append(epsilon)
    rewards.append(total_reward)
```

## ğŸ“Š **Paso 3: VisualizaciÃ³n**

### ğŸ” EvoluciÃ³n de Epsilon (Îµ)

```python
plt.figure(figsize=(10, 4))
plt.plot(epsilon_values, color='orange')
plt.title("ğŸ“‰ EvoluciÃ³n del valor de Îµ (ExploraciÃ³n)")
plt.xlabel("Episodios")
plt.ylabel("Valor de Îµ")
plt.grid(True)
plt.show()
```

### ğŸ† Recompensas por episodio

```python
plt.figure(figsize=(10, 4))
plt.plot(rewards, color='green')
plt.title("ğŸ† Recompensas acumuladas por episodio")
plt.xlabel("Episodios")
plt.ylabel("Recompensa")
plt.grid(True)
plt.show()
```

## ğŸ§­ **Paso 4: Visualizar la PolÃ­tica Aprendida**

```python
actions_map = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}
policy = np.array([actions_map[np.argmax(row)] for row in q_table])
print("ğŸ§­ PolÃ­tica final aprendida:")
print(policy.reshape(4, 4))
```

## ğŸ“ **ReflexiÃ³n**

> âœ… Al principio, el agente **explora** mucho porque no tiene informaciÃ³n.
>
> âœ… Con el tiempo, **explotar** le da mejores resultados porque confÃ­a en su experiencia.
>
> ğŸ§­ Â¡AsÃ­ se entrena la inteligencia! Aprender a decidir cuÃ¡ndo arriesgar y cuÃ¡ndo actuar con certeza.

ğŸ“Œ **Preguntas para reforzar** :

- Â¿QuÃ© pasarÃ­a si nunca disminuyÃ©ramos el valor de Îµ?
- Â¿QuÃ© observas en la polÃ­tica aprendida?
- Â¿PodrÃ­as adaptar esto a una situaciÃ³n real como un GPS o un robot mÃ³vil?