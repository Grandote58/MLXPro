{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa7240a",
   "metadata": {},
   "source": [
    "#  Mapa de Decisiones del Agente: Explorando un MDP \n",
    "\n",
    "##  Objetivo\n",
    "Explorar los fundamentos de un **Proceso de Decisi贸n de Markov (MDP)** a trav茅s de una simulaci贸n visual y matem谩tica. El estudiante aprender谩 a:\n",
    "- Representar los estados, acciones, recompensas y transiciones de un MDP.\n",
    "- Construir una tabla de transici贸n probabil铆stica.\n",
    "- Visualizar el comportamiento del sistema como un grafo.\n",
    "\n",
    " Ideal para estudiantes de pregrado y posgrado en cursos de Machine Learning o Inteligencia Artificial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77913f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instalaci贸n de librer铆as necesarias\n",
    "!pip install networkx matplotlib --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e357c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Importaci贸n de librer铆as\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80a841",
   "metadata": {},
   "source": [
    "##  Definimos nuestro MDP\n",
    "\n",
    "El entorno tiene 3 estados:\n",
    "- S0: Inicio\n",
    "- S1: Trampa\n",
    "- S2: Meta\n",
    "\n",
    "El agente puede:\n",
    "- A0: Avanzar\n",
    "- A1: Esperar\n",
    "\n",
    "Las probabilidades de transici贸n est谩n dadas por la funci贸n P(s'|s,a) y se definen en la matriz `P`.\n",
    "\n",
    "Los valores de recompensa se asignan solo al alcanzar la Meta (+1) o caer en la Trampa (-1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc445d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estados: S0, S1, S2\n",
    "# Acciones: A0 (Avanzar), A1 (Esperar)\n",
    "\n",
    "states = ['S0', 'S1', 'S2']\n",
    "actions = ['Avanzar', 'Esperar']\n",
    "\n",
    "# P[s][a] = [(next_state, probabilidad, recompensa)]\n",
    "P = {\n",
    "    'S0': {\n",
    "        'Avanzar': [('S1', 0.7, 0), ('S2', 0.3, 1)],\n",
    "        'Esperar': [('S0', 1.0, 0)]\n",
    "    },\n",
    "    'S1': {\n",
    "        'Avanzar': [('S1', 1.0, -1)],\n",
    "        'Esperar': [('S1', 1.0, -1)]\n",
    "    },\n",
    "    'S2': {\n",
    "        'Avanzar': [('S2', 1.0, 0)],\n",
    "        'Esperar': [('S2', 1.0, 0)]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66c845",
   "metadata": {},
   "source": [
    "##  Visualizamos el MDP como un grafo\n",
    "\n",
    "Usamos `networkx` para representar los estados como nodos y las transiciones como aristas con sus respectivas probabilidades y recompensas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44efe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear grafo dirigido\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# A帽adir nodos y aristas\n",
    "for s in P:\n",
    "    for a in P[s]:\n",
    "        for dest, prob, reward in P[s][a]:\n",
    "            label = f\"{a}\\nP={prob}, R={reward}\"\n",
    "            G.add_edge(s, dest, label=label)\n",
    "\n",
    "# Posiciones para los nodos\n",
    "pos = {'S0': (0, 0), 'S1': (1, -1), 'S2': (1, 1)}\n",
    "\n",
    "# Dibujar nodos\n",
    "plt.figure(figsize=(8,6))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='lightblue')\n",
    "nx.draw_networkx_labels(G, pos, font_size=12)\n",
    "\n",
    "# Dibujar aristas\n",
    "edges = G.edges(data=True)\n",
    "edge_labels = {(u, v): d['label'] for u, v, d in edges}\n",
    "nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10)\n",
    "\n",
    "plt.title(\" MDP: Mapa de Decisiones del Agente\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a390d25",
   "metadata": {},
   "source": [
    "##  Reflexi贸n Final\n",
    "\n",
    "Este MDP muestra c贸mo un agente puede tener caminos con distintos riesgos y recompensas.\n",
    "\n",
    " La probabilidad define la incertidumbre.  \n",
    " Las recompensas definen el objetivo.  \n",
    " Este mapa ayuda al agente a decidir cu谩l acci贸n es mejor en cada estado.\n",
    "\n",
    " En m贸dulos posteriores podr谩s usar estas estructuras para entrenar pol铆ticas 贸ptimas con Q-Learning o Dynamic Programming.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}