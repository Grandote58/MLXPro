# ğŸ§  Actividad eLearning: **â€œEl Explorador Curioso ğŸ§­â€**

### ğŸ¯ Objetivos de Aprendizaje

- Comprender el dilema **exploraciÃ³n vs. explotaciÃ³n** en Aprendizaje por Reforzamiento.
- Implementar el algoritmo **Îµ-greedy** y observar su impacto en el rendimiento del agente.
- Visualizar cÃ³mo el valor de Îµ\varepsilonÎµ afecta la toma de decisiones.
- Utilizar un entorno realista y visual de un laberinto con obstÃ¡culos.

### ğŸ“¦ LibrerÃ­as necesarias

```python
!pip install gymnasium matplotlib numpy --quiet
```

ğŸ“¦ LibrerÃ­as necesarias

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import random
```

### ğŸ—ºï¸ Dataset / Entorno utilizado

Utilizaremos el entorno educativo `FrozenLake-v1` (modo sin resbalones) del repositorio **Farama Gymnasium**:

ğŸ“¦ El entorno simula un lago congelado con agujeros, y el agente debe llegar a la meta sin caer.

## ğŸªœ Paso a Paso Detallado

### ğŸ”¹ Paso 1: Inicializar entorno y Q-table

```python
env = gym.make("FrozenLake-v1", is_slippery=False)
state_space = env.observation_space.n
action_space = env.action_space.n

print(f"Estados: {state_space} | Acciones: {action_space}")
q_table = np.zeros((state_space, action_space))
```

### ğŸ”¹ Paso 2: Definir hiperparÃ¡metros y estructuras

```python
alpha = 0.8        # tasa de aprendizaje
gamma = 0.95       # descuento de futuro
epsilon = 1.0      # valor inicial de exploraciÃ³n
min_epsilon = 0.01
decay_rate = 0.005
episodes = 500
max_steps = 100

rewards = []
epsilon_values = []
```

### ğŸ”¹ Paso 3: Entrenar el agente con estrategia Îµ-greedy

```python
for ep in range(episodes):
    state, _ = env.reset()
    total_reward = 0

    for _ in range(max_steps):
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # ğŸ” Explora
        else:
            action = np.argmax(q_table[state])  # ğŸ§  Explota

        new_state, reward, terminated, truncated, _ = env.step(action)

        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[new_state]) - q_table[state, action])
        state = new_state
        total_reward += reward

        if terminated or truncated:
            break

    epsilon = max(min_epsilon, epsilon * np.exp(-decay_rate * ep))
    epsilon_values.append(epsilon)
    rewards.append(total_reward)
```

### ğŸ”¹ Paso 4: Visualizar cÃ³mo evoluciona la exploraciÃ³n

```python
plt.plot(epsilon_values)
plt.title("ğŸ“‰ EvoluciÃ³n del valor de Îµ (Epsilon)")
plt.xlabel("Episodios")
plt.ylabel("ExploraciÃ³n")
plt.grid(True)
plt.show()
```

### ğŸ”¹ Paso 5: Visualizar el aprendizaje del agente

```python
plt.plot(rewards)
plt.title("ğŸ† Recompensas acumuladas por episodio")
plt.xlabel("Episodios")
plt.ylabel("Recompensa")
plt.grid(True)
plt.show()
```

### ğŸ”¹ Paso 6: PolÃ­tica aprendida del agente

```python
actions_map = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}
policy = np.array([actions_map[np.argmax(row)] for row in q_table])
print("ğŸ§­ PolÃ­tica aprendida:")
print(policy.reshape(4, 4))
```

### ğŸ“ ReflexiÃ³n

> â€œUn agente inteligente aprende a **equilibrar entre probar lo desconocido y usar lo aprendido**.
>  Gracias al algoritmo Îµ-greedy, el Explorador Curioso va disminuyendo su curiosidad a medida que mejora su conocimiento.
>
>  Este principio es usado en robots autÃ³nomos, videojuegos y sistemas de recomendaciÃ³n.
>  Â¡Hoy tÃº tambiÃ©n has aprendido a equilibrar la exploraciÃ³n con sabidurÃ­a! ğŸš€â€