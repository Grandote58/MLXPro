# **ğŸ§  Actividad: "Mi Primer Agente Inteligente: Â¡Rumbo a la Meta! ğŸ¯"**

### ğŸ‘©â€ğŸ“ Dirigido a:

Estudiantes de pregrado y posgrado en cursos de IntroducciÃ³n al Machine Learning o Inteligencia Artificial.

## ğŸ¯ Objetivos de Aprendizaje

1. Comprender los fundamentos del **Aprendizaje por Reforzamiento (RL)** y la interacciÃ³n *agente â†” ambiente*.
2. Implementar un agente simple con el algoritmo **Q-Learning**.
3. Analizar la evoluciÃ³n del agente a travÃ©s de recompensas acumuladas.
4. Interpretar los resultados visualmente y ajustar parÃ¡metros del aprendizaje.

## ğŸ› ï¸ LibrerÃ­as necesarias

```python
!pip install gymnasium[classic_control] --quiet
```

### ğŸ› ï¸ LibrerÃ­as necesarias

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import random
import time
from IPython.display import clear_output
```

## ğŸ“ Dataset Utilizado

Utilizaremos el entorno **FrozenLake-v1** del paquete `Gymnasium`, un ambiente estÃ¡ndar en RL.

 ğŸ“„ **Repositorio oficial:** https://gymnasium.farama.org/environments/toy_text/frozen_lake/

Este entorno representa un lago congelado donde el agente debe **llegar a una meta** sin caer en los agujeros.

## ğŸ§ª Paso a Paso Detallado

### ğŸ”¹ Paso 1: Crear el entorno

```python
env = gym.make("FrozenLake-v1", is_slippery=False)
state_space = env.observation_space.n
action_space = env.action_space.n

print(f"ğŸ”¢ Estados posibles: {state_space}")
print(f"ğŸ® Acciones posibles: {action_space}")
```

> âœ… `is_slippery=False` hace que el entorno sea determinista, facilitando el aprendizaje inicial.

### ğŸ”¹ Paso 2: Inicializar la tabla Q

```python
q_table = np.zeros((state_space, action_space))
print("ğŸ“Š Tabla Q inicializada:")
print(q_table)
```

### ğŸ”¹ Paso 3: Definir los hiperparÃ¡metros

```python
# NÃºmero de episodios de entrenamiento
episodes = 1000
max_steps = 100

# HiperparÃ¡metros de Q-Learning
learning_rate = 0.8
discount_factor = 0.95

# ExploraciÃ³n vs ExplotaciÃ³n
epsilon = 1.0       # exploraciÃ³n inicial
min_epsilon = 0.01
decay_rate = 0.005
```

### ğŸ”¹ Paso 4: Entrenamiento del agente

```python
rewards = []

for episode in range(episodes):
    state, _ = env.reset()
    total_rewards = 0

    for step in range(max_steps):
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # exploraciÃ³n
        else:
            action = np.argmax(q_table[state])  # explotaciÃ³n

        next_state, reward, terminated, truncated, _ = env.step(action)

        q_table[state, action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state]) - q_table[state, action])

        state = next_state
        total_rewards += reward

        if terminated or truncated:
            break

    epsilon = max(min_epsilon, epsilon * np.exp(-decay_rate * episode))
    rewards.append(total_rewards)

print("âœ… Entrenamiento finalizado")
```

### ğŸ”¹ Paso 5: Visualizar recompensas por episodio

```python
plt.plot(rewards)
plt.title("ğŸ¯ Recompensas por episodio")
plt.xlabel("Episodios")
plt.ylabel("Recompensa")
plt.grid(True)
plt.show()
```

### ğŸ”¹ Paso 6: Probar el agente entrenado

```python
for i in range(3):
    state, _ = env.reset()
    print(f"\nğŸš€ Episodio {i+1}")
    time.sleep(1)

    for step in range(max_steps):
        clear_output(wait=True)
        env.render()
        time.sleep(0.5)

        action = np.argmax(q_table[state])
        state, reward, terminated, truncated, _ = env.step(action)

        if terminated or truncated:
            clear_output(wait=True)
            env.render()
            if reward == 1:
                print("ğŸ‰ Â¡El agente alcanzÃ³ la meta!")
            else:
                print("ğŸ’§ Â¡El agente cayÃ³ en el agua!")
            time.sleep(2)
            break
env.close()
```

## ğŸ“ ReflexiÃ³n Final



> ğŸ“Œ â€œEste ejercicio muestra cÃ³mo un agente sin conocimiento previo puede aprender a tomar decisiones Ã³ptimas simplemente interactuando con su entorno. Usando Q-Learning, el agente ajusta su comportamiento con base en recompensas, un paso crucial hacia sistemas inteligentes autÃ³nomos.â€

