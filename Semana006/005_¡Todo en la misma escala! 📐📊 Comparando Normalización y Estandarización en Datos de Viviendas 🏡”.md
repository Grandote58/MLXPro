# **Â¡Todo en la misma escala! ğŸ“ğŸ“Š Comparando NormalizaciÃ³n y EstandarizaciÃ³n en Datos de Viviendas ğŸ¡**

### ğŸ¯ **Objetivo**

Aplicar **NormalizaciÃ³n** y **EstandarizaciÃ³n** sobre dos variables del dataset **California Housing**, visualizar los cambios y entrenar un modelo de regresiÃ³n lineal para predecir el precio medio de una vivienda. Se evidenciarÃ¡ cÃ³mo el escalado mejora el rendimiento y estabilidad del modelo.

### ğŸ“¦ **Dataset**

Se utilizarÃ¡ el dataset **California Housing** de `sklearn.datasets`.

ğŸ”— URL oficial:
 https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html

### ğŸ§ª **Paso a Paso**

#### ğŸ”¹ Paso 1: Importar librerÃ­as necesarias ğŸ§°

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
```

#### ğŸ”¹ Paso 2: Cargar el dataset ğŸ¡

```python
# Cargar datos
data = fetch_california_housing(as_frame=True)
df = data.frame

# Visualizar las primeras filas
df.head()
```

#### ğŸ”¹ Paso 3: Seleccionar y visualizar las variables originales ğŸ“Š

```python
# Seleccionar dos variables: AveRooms y AveOccup
features = df[['AveRooms', 'AveOccup']]

# Graficar distribuciÃ³n original
features.hist(figsize=(10, 4), bins=30, color='lightblue')
plt.suptitle("ğŸ” DistribuciÃ³n original de AveRooms y AveOccup")
plt.show()
```

#### ğŸ”¹ Paso 4: Aplicar NormalizaciÃ³n y EstandarizaciÃ³n âš–ï¸

```python
# NormalizaciÃ³n
minmax_scaler = MinMaxScaler()
features_norm = pd.DataFrame(minmax_scaler.fit_transform(features), columns=features.columns)

# EstandarizaciÃ³n
standard_scaler = StandardScaler()
features_std = pd.DataFrame(standard_scaler.fit_transform(features), columns=features.columns)
```

#### ğŸ”¹ Paso 5: Visualizar escalado con grÃ¡ficas ğŸ¨

```python
# Graficar resultados de escalado
plt.figure(figsize=(12, 4))

# NormalizaciÃ³n
plt.subplot(1, 2, 1)
plt.hist(features_norm, bins=30, stacked=True)
plt.title("ğŸ”µ NormalizaciÃ³n (MinMaxScaler)")

# EstandarizaciÃ³n
plt.subplot(1, 2, 2)
plt.hist(features_std, bins=30, stacked=True)
plt.title("ğŸŸ  EstandarizaciÃ³n (StandardScaler)")

plt.tight_layout()
plt.show()
```

#### ğŸ”¹ Paso 6: Entrenar modelo sin escalado vs con escalado ğŸ“ˆ

```python
# Variable objetivo
y = df['MedHouseVal']

# Modelos con variables originales
X_train_o, X_test_o, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42)
model_o = LinearRegression().fit(X_train_o, y_train)
pred_o = model_o.predict(X_test_o)
r2_o = r2_score(y_test, pred_o)

# Modelos con normalizaciÃ³n
X_train_n, X_test_n, _, _ = train_test_split(features_norm, y, test_size=0.2, random_state=42)
model_n = LinearRegression().fit(X_train_n, y_train)
pred_n = model_n.predict(X_test_n)
r2_n = r2_score(y_test, pred_n)

# Modelos con estandarizaciÃ³n
X_train_s, X_test_s, _, _ = train_test_split(features_std, y, test_size=0.2, random_state=42)
model_s = LinearRegression().fit(X_train_s, y_train)
pred_s = model_s.predict(X_test_s)
r2_s = r2_score(y_test, pred_s)

# Resultados
print(f"ğŸ“Š Sin escalado - RÂ²: {r2_o:.4f}")
print(f"ğŸ”µ Normalizado - RÂ²: {r2_n:.4f}")
print(f"ğŸŸ  Estandarizado - RÂ²: {r2_s:.4f}")
```

### ğŸ’¬ **ExplicaciÃ³n**

ğŸ” El escalado es como dar las mismas oportunidades a todas las variables. Si una tiene un rango de 0â€“1000 y otra de 0â€“10, el modelo le darÃ¡ mÃ¡s peso a la primera... aunque no sea mÃ¡s importante.

ğŸ§ª En esta prÃ¡ctica:

- Usamos dos variables (`AveRooms` y `AveOccup`) para simplificar la visualizaciÃ³n.
- Aplicamos dos tÃ©cnicas:
  - **MinMaxScaler (NormalizaciÃ³n)**: ajusta los valores entre 0 y 1.
  - **StandardScaler (EstandarizaciÃ³n)**: centra la media en 0 y ajusta la desviaciÃ³n a 1.

ğŸ“Š Luego de aplicar cada tÃ©cnica:

- **Visualizamos sus efectos** con histogramas.
- **Entrenamos modelos** con los datos escalados y comparamos su desempeÃ±o con el modelo sin escalado.

ğŸ“ˆ Como se observa en los resultados, **el escalado puede mejorar el rendimiento o la estabilidad**, especialmente en modelos que utilizan operaciones matemÃ¡ticas con distancias o pesos.

### âœ… **ReflexiÃ³n final**

> El preprocesamiento es mÃ¡s que una etapa tÃ©cnica: es **la base para que los modelos aprendan correctamente**.
>  Escalar las variables es como nivelar el terreno antes de construir: garantiza un entrenamiento mÃ¡s justo, mÃ¡s rÃ¡pido y mÃ¡s preciso. ğŸ§±âš–ï¸ğŸ“

### ğŸ’¡ Tips :

- Usa `MinMaxScaler` si tus datos no tienen valores extremos (outliers).
- Usa `StandardScaler` si esperas una distribuciÃ³n normal o tienes outliers.
- Aplica escalado **despuÃ©s** de la divisiÃ³n Train/Test para evitar fuga de datos.